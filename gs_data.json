{"container_type": "Author", "filled": ["basics", "publications", "indices", "counts"], "scholar_id": "dBVSZWAAAAAJ", "source": "AUTHOR_PROFILE_PAGE", "name": "Kai (Alan) Wang", "affiliation": "University of Toronto", "organization": 8515235176732148308, "interests": ["Multimodal Learning", "Generative Models", "Multimodal LLM"], "email_domain": "@mail.utoronto.ca", "citedby": 242, "publications": {"dBVSZWAAAAAJ:u-x6o8ySG0sC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "TSTNN: Two-stage transformer based neural network for speech enhancement in the time domain", "pub_year": "2021"}, "filled": false, "author_pub_id": "dBVSZWAAAAAJ:u-x6o8ySG0sC", "num_citations": 175, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=938644009392487728,8221893802366853758", "cites_id": ["938644009392487728", "8221893802366853758"]}, "dBVSZWAAAAAJ:ufrVoPGSRksC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Mmlu-pro: A more robust and challenging multi-task language understanding benchmark", "pub_year": "2024"}, "filled": false, "author_pub_id": "dBVSZWAAAAAJ:ufrVoPGSRksC", "num_citations": 32, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=2666986522064548248,5485551790807912608", "cites_id": ["2666986522064548248", "5485551790807912608"]}, "dBVSZWAAAAAJ:d1gkVwhDpl0C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Caunet: Context-aware u-net for speech enhancement in time domain", "pub_year": "2021"}, "filled": false, "author_pub_id": "dBVSZWAAAAAJ:d1gkVwhDpl0C", "num_citations": 22, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=1036027931645055967", "cites_id": ["1036027931645055967"]}, "dBVSZWAAAAAJ:2osOgNQ5qMEC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Cptnn: Cross-parallel transformer neural network for time-domain speech enhancement", "pub_year": "2022"}, "filled": false, "author_pub_id": "dBVSZWAAAAAJ:2osOgNQ5qMEC", "num_citations": 8, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=871235213731194962", "cites_id": ["871235213731194962"]}, "dBVSZWAAAAAJ:UebtZRa9Y70C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "HARWE: A multi-modal large-scale dataset for context-aware human activity recognition in smart working environments", "pub_year": "2024"}, "filled": false, "author_pub_id": "dBVSZWAAAAAJ:UebtZRa9Y70C", "num_citations": 1, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=1055030511760264357", "cites_id": ["1055030511760264357"]}, "dBVSZWAAAAAJ:hqOjcs7Dif8C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "VideoScore: Building Automatic Metrics to Simulate Fine-grained Human Feedback for Video Generation", "pub_year": "2024"}, "filled": false, "author_pub_id": "dBVSZWAAAAAJ:hqOjcs7Dif8C", "num_citations": 1, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=12320187184887501593", "cites_id": ["12320187184887501593"]}, "dBVSZWAAAAAJ:WF5omc3nYNoC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Towards Efficient Audio-Visual Learners via Empowering Pre-trained Vision Transformers with Cross-Modal Adaptation", "pub_year": "2024"}, "filled": false, "author_pub_id": "dBVSZWAAAAAJ:WF5omc3nYNoC", "num_citations": 1, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=10914536817133681931", "cites_id": ["10914536817133681931"]}, "dBVSZWAAAAAJ:UeHWp8X0CEIC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "SE-DPTUNet: Dual-Path Transformer based U-Net for Speech Enhancement", "pub_year": "2022"}, "filled": false, "author_pub_id": "dBVSZWAAAAAJ:UeHWp8X0CEIC", "num_citations": 1, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=4900843064359868201", "cites_id": ["4900843064359868201"]}, "dBVSZWAAAAAJ:Tyk-4Ss8FVUC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "DBAUNet: Dual-branch attention U-Net for time-domain speech enhancement", "pub_year": "2022"}, "filled": false, "author_pub_id": "dBVSZWAAAAAJ:Tyk-4Ss8FVUC", "num_citations": 1, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=5515208363236376632", "cites_id": ["5515208363236376632"]}, "dBVSZWAAAAAJ:_FxGoFyzp5QC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "AV-DiT: Efficient Audio-Visual Diffusion Transformer for Joint Audio and Video Generation", "pub_year": "2024"}, "filled": false, "author_pub_id": "dBVSZWAAAAAJ:_FxGoFyzp5QC", "num_citations": 0}, "dBVSZWAAAAAJ:eQOLeE2rZwMC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "MOMA: Mixture-of-Modality-Adaptations for Transferring Knowledge from Image Models Towards Efficient Audio-Visual Action Recognition", "pub_year": "2024"}, "filled": false, "author_pub_id": "dBVSZWAAAAAJ:eQOLeE2rZwMC", "num_citations": 0}, "dBVSZWAAAAAJ:W7OEmFMy1HYC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "SEformer: Dual-Path Conformer Neural Network is a Good Speech Denoiser", "pub_year": "2023"}, "filled": false, "author_pub_id": "dBVSZWAAAAAJ:W7OEmFMy1HYC", "num_citations": 0}, "dBVSZWAAAAAJ:zYLM7Y9cAGgC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "SE-Mixer: Towards an Efficient Attention-free Neural Network for Speech Enhancement", "pub_year": "2022"}, "filled": false, "author_pub_id": "dBVSZWAAAAAJ:zYLM7Y9cAGgC", "num_citations": 0}, "dBVSZWAAAAAJ:YsMSGLbcyi4C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Optimization over Client Selection in Efficient Federated Learning"}, "filled": false, "author_pub_id": "dBVSZWAAAAAJ:YsMSGLbcyi4C", "num_citations": 0}}, "citedby5y": 242, "hindex": 4, "hindex5y": 4, "i10index": 3, "i10index5y": 3, "cites_per_year": {"2021": 8, "2022": 50, "2023": 85, "2024": 96}, "updated": "2024-10-10 08:24:47.725829"}