
# ðŸ“ Selected Publications
> ( <sup>*</sup> equal contribution)

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ArXiv</div><img src='./images/AV-DiT.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[AV-DiT: Efficient Audio-Visual Diffusion Transformer for Joint Audio and Video Generation](https://arxiv.org/pdf/2406.07686) 

**Kai Wang**, Shijian Deng, Jing Shi, Dimitrios Hatzinakos, Yapeng Tian. 

**Under Review**

- We design an efficient audio-visual diffusion transformer generate high-quality, realistic videos with both visual and audio tracks.

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2024</div><img src='./images/mmlu_pro.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[MMLU-Pro: A more robust and challenging multi-task language understanding benchmark](https://arxiv.org/pdf/2406.01574) 

Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, **Kai Wang**, Alex Zhuang, Rongqi Fan, Xiang Yue, Wenhu Chen.

 **NeurIPS 2024 (Spotlight)**


</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">EMNLP 2024</div><img src='./images/videoscore.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[VideoScore: Building Automatic Metrics to Simulate Fine-grained Human Feedback for Video Generation](https://arxiv.org/abs/2406.15252) 

Xuan He, Dongfu Jiang, Ge Zhang, Max Ku, Achint Soni, Sherman Siu, Haonan Chen, Abhranil Chandra, Ziyan Jiang, Aaran Arulraj, **Kai Wang**, Quy Duc Do, Yuansheng Ni, Bohan Lyu, Yaswanth Narsupalli, Rongqi Fan, Zhiheng Lyu, Yuchen Lin, Wenhu Chen

 **EMNLP 2024 (Main)**


</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2024</div><img src='./images/STG-CMA.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Towards Efficient Audio-Visual Learners via Empowering Pre-trained Vision Transformers with Cross-Modal Adaptation](https://openaccess.thecvf.com/content/CVPR2024W/MMFM/papers/Wang_Towards_Efficient_Audio-Visual_Learners_via_Empowering_Pre-trained_Vision_Transformers_with_CVPRW_2024_paper.pdf) 

**Kai Wang**,Yapeng Tian, Dimitrios Hatzinakos. 

**CVPR 2024 Workshop**

- We propose a Spatial-Temporal-Global Cross-Modal Adaptation (STG-CMA) to gradually equip the frozen ViTs with the capability for learning audio-visual representation.

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Pattern Recognition Letter</div><img src='./images/harwe.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[HARWE: A multi-modal large-scale dataset for context-aware human activity recognition in smart working environments](https://www.sciencedirect.com/science/article/abs/pii/S0167865524001879) 

Alireza Esmaeilzehi<sup>*</sup>, Ensieh Khazaei<sup>*</sup>, **Kai Wang<sup>*</sup>**, Navjot Kaur Kalsi, Pai Chet Ng, Huan Liu, Yuanhao Yu, Dimitrios Hatzinakos, Konstantinos Plataniotis.

**Pattern Recognition Letter**

- We propose a novel dataset for the task of human activity recognition, in which the labels are specified for the working environments.

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICASSP 2024</div><img src='./images/moma.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[MoMA: Mixture-of-Modality-Adaptations for Transferring Knowledge from Image Models Towards Efficient Audio-Visual Action Recognition](https://ieeexplore.ieee.org/abstract/document/10446627) 

**Kai Wang**, Dimitrios Hatzinakos. 

**ICASSP 2024 (Oral)**

- We propose a novel parameter-efficient scheme called Mixture-of-Modality-Adaptations (MoMA) for audio-visual action recognition.

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">APSIPA 2023</div><img src='./images/seformer.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[SEformer: Dual-Path Conformer Neural Network is a Good Speech Denoiser](https://ieeexplore.ieee.org/abstract/document/10317376) 

**Kai Wang**, Dimitrios Hatzinakos. 

**APSIPA 2023 (Oral)**

- We propose the SEformer, an efficient dual-path conformer neural network for speech enhancement.

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">IWAENC 2022</div><img src='./images/cptnn.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Cptnn: Cross-parallel transformer neural network for time-domain speech enhancement](https://ieeexplore.ieee.org/abstract/document/9914777) 

**Kai Wang**, Bengbeng He and Wei-Ping Zhu. 

**IWAENC 2022**


</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ISCAS 2021</div><img src='./images/caunet.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[CAUNet: Context-Aware U-Net for Speech Enhancement in Time Domain](https://ieeexplore.ieee.org/abstract/document/9401787) 

**Kai Wang**, Bengbeng He and Wei-Ping Zhu. 

**ISCAS 2021**

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICASSP 2021</div><img src='./images/tstnn.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[TSTNN: Two-stage transformer based neural network for speech enhancement in the time domain](https://arxiv.org/pdf/2103.09963) 

**Kai Wang**, Bengbeng He and Wei-Ping Zhu. 

**ICASSP 2021**


</div>
</div>